{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_rows = 35 \n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 1- All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    31899\n",
      "1    21076\n",
      "2     4272\n",
      "Name: target, dtype: int64\n",
      "0    27114\n",
      "1    17914\n",
      "2     3631\n",
      "Name: target, dtype: int64 0    4785\n",
      "1    3162\n",
      "2     641\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/processed/WaterUpdated.csv').drop('id', axis = 1)\n",
    "df.target.replace({'functional': 0, 'non functional': 1, 'functional needs repair': 2}, inplace = True)\n",
    "new_df = df\n",
    "new_df = df.join(pd.get_dummies(df.basin, prefix = 'basin'))\n",
    "new_df = new_df.join(pd.get_dummies(df.extraction_type, prefix = 'extract'))\n",
    "new_df = new_df.join(pd.get_dummies(df.quantity, prefix = 'quantity'))\n",
    "new_df = new_df.join(pd.get_dummies(df.water_quality, prefix = 'quality'))\n",
    "new_df = new_df.join(pd.get_dummies(df.source, prefix = 'source'))\n",
    "new_df = new_df.join(pd.get_dummies(df.waterpoint_type, prefix = 'waterpoint'))\n",
    "\n",
    "\n",
    "unique_basin = [f'basin_{i}' for i in df.basin.unique()]\n",
    "unique_extract = [f'extract_{i}' for i in df.extraction_type.unique()]\n",
    "unique_waterpoint = [f'waterpoint_{i}' for i in df.waterpoint_type.unique()]\n",
    "unique_source = [f'source_{i}' for i in df.source.unique() if i != 'unknown']\n",
    "unique_quality = [f'quality_{i}' for i in df.water_quality.unique() if i != 'unknown']\n",
    "unique_quantity = [f'quantity_{i}' for i in df.quantity.unique() if i != 'unknown']\n",
    "\n",
    "col = ['amount_tsh', 'gps_height', 'population', 'permit', 'time_passed', 'target']\n",
    "col = col + unique_basin + unique_extract + unique_waterpoint + unique_source + unique_quality + unique_quantity \n",
    "new_df = new_df[col]\n",
    "\n",
    "print(new_df.target.value_counts())\n",
    "\n",
    "\n",
    "\n",
    "X = new_df[col]\n",
    "y = new_df[['target']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, stratify = X.target.values, train_size = .85, random_state = 0)\n",
    "\n",
    "\n",
    "print(y_train.target.value_counts(), y_test.target.value_counts())\n",
    "\n",
    "# func_df = x_train[x_train.target == 0]\n",
    "# non_func = x_train[x_train.target ==1]\n",
    "# needs_repair = x_train[x_train.target == 2]\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.drop('target', axis = 1)\n",
    "x_test = x_test.drop('target', axis  = 1)\n",
    "\n",
    "smote= SMOTE()\n",
    "x_train_resamp, y_train_resamp = smote.fit_sample(x_train.values, y_train.target.values)\n",
    "\n",
    "x_train_resamp = pd.DataFrame(x_train_resamp, columns = x_train.columns)\n",
    "y_train_resamp = pd.DataFrame(y_train_resamp, columns = ['target'])\n",
    "pickle.dump(y_test, open('../../data/processed/pickles/SMOTE_y_test.p', 'wb'))\n",
    "pickle.dump(y_train_resamp, open('../../data/processed/pickles/SMOTE_y_train.p', 'wb'))\n",
    "pickle.dump(x_train_resamp, open('../../data/processed/pickles/SMOTE_x_train.p', 'wb'))\n",
    "pickle.dump(x_test, open('../../data/processed/pickles/SMOTE_x_test.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = resampled_concat\n",
    "# y = resampled_concat[['target']]\n",
    "\n",
    "# print(len(X), len(new_df))\n",
    "\n",
    "# pickle.dump(X.drop('target', axis = 1), open('../../data/processed/pickles/X.p', 'wb'))\n",
    "# pickle.dump(y, open('../../data/processed/pickles/y.p', 'wb'))\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X,y, stratify = X.target, random_state = 10, train_size = .85)\n",
    "# print(y_train.target.value_counts(), y_test.target.value_counts())\n",
    "# x_train = x_train.drop('target', axis =1)\n",
    "# x_test = x_test.drop('target', axis =1)\n",
    "\n",
    "# pickle.dump(y_test, open('../../data/processed/pickles/y_test.p', 'wb'))\n",
    "# pickle.dump(y_train, open('../../data/processed/pickles/y_train.p', 'wb'))\n",
    "# pickle.dump(x_train, open('../../data/processed/pickles/x_train.p', 'wb'))\n",
    "# pickle.dump(x_test, open('../../data/processed/pickles/x_test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
