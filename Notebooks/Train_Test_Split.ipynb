{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_rows = 35 \n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 1- All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57247, 54)\n",
      "0    27114\n",
      "1    21545\n",
      "Name: target, dtype: int64\n",
      "38090\n",
      "1    21545\n",
      "0    16545\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/WaterUpdated.csv').drop('id', axis = 1)\n",
    "df.target.replace({'functional': 0, 'non functional': 1, 'functional needs repair': 1}, inplace = True)\n",
    "new_df = df\n",
    "new_df = df.join(pd.get_dummies(df.basin, prefix = 'basin'))\n",
    "new_df = new_df.join(pd.get_dummies(df.extraction_type, prefix = 'extract'))\n",
    "new_df = new_df.join(pd.get_dummies(df.quantity, prefix = 'quantity'))\n",
    "new_df = new_df.join(pd.get_dummies(df.water_quality, prefix = 'quality'))\n",
    "new_df = new_df.join(pd.get_dummies(df.source, prefix = 'source'))\n",
    "new_df = new_df.join(pd.get_dummies(df.waterpoint_type, prefix = 'waterpoint'))\n",
    "\n",
    "\n",
    "unique_basin = [f'basin_{i}' for i in df.basin.unique()]\n",
    "unique_extract = [f'extract_{i}' for i in df.extraction_type.unique()]\n",
    "unique_waterpoint = [f'waterpoint_{i}' for i in df.waterpoint_type.unique()]\n",
    "unique_source = [f'source_{i}' for i in df.source.unique()]\n",
    "unique_quality = [f'quality_{i}' for i in df.water_quality.unique() ]\n",
    "unique_quantity = [f'quantity_{i}' for i in df.quantity.unique()]\n",
    "\n",
    "col = ['amount_tsh', 'gps_height', 'population', 'permit', 'time_passed', 'target']\n",
    "col = col + unique_basin + unique_extract + unique_waterpoint + unique_source + unique_quality + unique_quantity \n",
    "new_df = new_df[col]\n",
    "\n",
    "\n",
    "X = new_df\n",
    "y = new_df[['target']]\n",
    "\n",
    "print(X.shape)\n",
    "pickle.dump(X.drop('target', axis = 1), open('../data/processed/pickles/X.p', 'wb'))\n",
    "pickle.dump(y, open('../data/processed/pickles/y.p', 'wb'))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, stratify = X.target, random_state = 10, train_size = .85)\n",
    "\n",
    "\n",
    "func_x_train = x_train[x_train.target == 0]\n",
    "repair_x_train = x_train[x_train.target ==1]\n",
    "\n",
    "print(x_train.target.value_counts())\n",
    "\n",
    "# resamp_x_train = resample(repair_x_train, n_samples = len(func_x_train), random_state = 10, replace = True)\n",
    "# resampled_x_train = pd.concat([func_x_train, resamp_x_train])\n",
    "\n",
    "resamp_x_train = resample(func_x_train, n_samples = len(repair_x_train)-5000, random_state = 10, replace = True)\n",
    "resampled_x_train = pd.concat([repair_x_train, resamp_x_train])\n",
    "\n",
    "print(len(resampled_x_train))\n",
    "print(resampled_x_train.target.value_counts())\n",
    "x_train = resampled_x_train\n",
    "y_train = resampled_x_train[['target']]\n",
    "# # print(len(y_train), y_train.target.value_counts(), y_test.target.value_counts())\n",
    "\n",
    "\n",
    "x_train = x_train.drop('target', axis =1)\n",
    "x_test = x_test.drop('target', axis =1)\n",
    "\n",
    "# print(y_train.target.value_counts(), y_test.target.value_counts())\n",
    "\n",
    "pickle.dump(y_test, open('../data/processed/pickles/y_test.p', 'wb'))\n",
    "pickle.dump(y_train, open('../data/processed/pickles/y_train.p', 'wb'))\n",
    "pickle.dump(x_train, open('../data/processed/pickles/x_train.p', 'wb'))\n",
    "pickle.dump(x_test, open('../data/processed/pickles/x_test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
